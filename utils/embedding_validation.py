# Emebdding Validation tasks for neural models


import os
import numpy as np
import pandas as pd
from nltk.corpus import stopwords, wordnet
from gensim.models import KeyedVectors
from scipy import spatial
import string
import random
from sklearn.cluster import KMeans
import requests
from io import StringIO
import timeit
from traceback import format_exc
import re
import pickle
from sklearn.linear_model import LogisticRegressionCV
from sklearn.feature_extraction.text import CountVectorizer


class EmbeddingValidation(object):


    """
    A class used to evaluate the embedding representations with a series of tasks


    Attributes
    ----------
    vectors_file : str
        the absolute path to the *.wordvectors file of the neural embedding model to be evaluated
    vector_size : int
        the size of the vectors generated by the neural embedding model

    Methods
    -------
    Relatedness(word_pairs: str, eval_metric: str, corr_metric: str)
        Implements word relatedness embedding intrinsic evaluator
    Categorization(category_file: str, n_categories: int, seed: str)
        Implements word categorization intrinsic evaluator
    Analogy(words: bool or str or None, bats: str or None, analogy_domain: list or None, sample: int, seed: int, metric: str, epsilon: float)
        Implements word analogy intrinsic evaluator
    SentimentAnalysis(labeled_dataset: str or pd.DataFrame, sample: int or None, seed: int, metrics: str, preprocessing: bool)
        Implements sentiment analysis extrinsic evaluator
    FrequencyCategory(sample_size: int, threshold_list: list, metrics: str)
        Implements frequency-rare word classification extrinsic evaluator
    
    """
    

    def __str__(self):
        
        return "EmbeddingValidation class"


    def __init__(self, vectors_file: str, vector_size: int):

        stopwd = stopwords.words("english")

        try:
            wv = KeyedVectors.load(f"{vectors_file}", mmap='r')
        except:
            print(format_exc())
            raise NameError(f"{vectors_file} is NOT a valid vector file, please provide one with '.wordvectors' extension!")
        
        print("... Word Vectors: DONE!")
        print("... Word Frequencies: DONE!")

        self.wv = wv
        self.vector_size = vector_size
        self.stopwd = stopwd


    def Relatedness(self, word_pairs: str, eval_metric='cosine', corr_metric='pearson'):
        
        start = timeit.default_timer()
        try:
            # Read file
            if os.path.isfile(word_pairs):
                with open(word_pairs, 'r') as f:
                    df = pd.read_csv(word_pairs, sep='	', names=['Word1', 'Word2', 'Word Relation GS'])
                    df.loc[:, 'Word1'] = df['Word1'].apply(lambda x: x.lower())
                    df.loc[:, 'Word2'] = df['Word2'].apply(lambda x: x.lower())
            else:
                raise NameError(f"{word_pairs} is NOT a valid argument, please provide the path to a word pairs file!")

            # Compute cosine similarity
            if eval_metric.lower() == 'cosine':
                for idx, row in df.iterrows():
                    if row[0].lower() in self.wv.index_to_key and row[1].lower() in self.wv.index_to_key:
                        df.loc[idx, 'Cosine Similarity'] = 1 - spatial.distance.cosine(self.wv[row[0]], self.wv[row[1]])
                    else:
                        print(f"Cosine Similarity CANNOT be computed for pair of words '{row[0]}' and '{row[1]}'")
                        continue

            # Compute correlation
            if corr_metric.lower() == 'pearson':
                correlation = df['Cosine Similarity'].corr(df['Word Relation GS'], method='pearson')
            elif corr_metric.lower() == 'spearman':
                correlation = df['Cosine Similarity'].corr(df['Word Relation GS'], method='spearman')

            self.df = df
            self.correlation = correlation

            end = timeit.default_timer()
            print(f"--> DONE {int((end - start))} second(s)!")

            return self.correlation

        except:
            print(format_exc())
            raise Exception


    def Categorization(self, category_file: str, n_categories: int, seed=100):
        
        start = timeit.default_timer()
        try:
            # Read file
            if os.path.isfile(category_file):
                with open(category_file, 'r') as f:
                    words_cat = pd.read_csv(category_file, sep="	", names=['word', 'category', 'score'])

                words_cat.loc[:, 'score'] = pd.to_numeric(words_cat['score'])
                words_cat = words_cat.loc[words_cat['score'] >= 0.75]
                words_cat.loc[:, 'word'] = words_cat['word'].apply(lambda x: wordnet.synset_from_pos_and_offset(x[0], int(x[1:])).name().split(".")[0])

                # Clean words
                words_cat.loc[:, 'word'] = words_cat['word'].apply(lambda x: x.lower().strip().translate(str.maketrans('', '', string.punctuation)))
                words_cat.loc[:, 'word'] = words_cat['word'].apply(lambda x: x if x in self.wv.index_to_key else None)
                words_cat = words_cat.loc[(words_cat['word'].notnull())]
                print("... Word Categories: DONE!")
                
                print(words_cat.shape)
                print(words_cat['category'].unique())

                dict_domains = {'Animals': 0, 
                                'Art, architecture, and archaeology': 1,
                                'Biology': 2,
                                'Business, economics, and finance': 3,
                                'Chemistry and mineralogy': 4,
                                'Computing': 5,
                                'Culture and society': 6,
                                'Education': 7,
                                'Engineering and technology': 8,
                                'Farming': 9,
                                'Food and drink': 10,
                                'Games and video games': 11,
                                'Geography and places': 12,
                                'Geology and geophysics': 13,
                                'Health and medicine': 14,
                                'Heraldry, honors, and vexillology': 15,
                                'History': 16,
                                'Language and linguistics': 17,
                                'Law and crime': 18,
                                'Literature and theatre': 19,
                                'Mathematics': 20,
                                'Media': 21,
                                'Meteorology': 22,
                                'Music': 23,
                                'Numismatics and currencies': 24,
                                'Philosophy and psychology': 25,
                                'Physics and astronomy': 26,
                                'Politics and government': 27,
                                'Religion, mysticism and mythology': 28,
                                'Royalty and nobility': 29,
                                'Sport and recreation': 30,
                                'Textile and clothing': 31,
                                'Transport and travel': 32,
                                'Warfare and defense': 33}

                # Sample dataset and categories
                random.seed(seed)
                domains = random.sample(list(words_cat.category.unique()), n_categories)
                df_domains = words_cat.loc[(words_cat['category'].isin(domains))]
                df_cat = pd.DataFrame(columns=['word'] + [f"d_{i}" for i in range(self.vector_size)] + ['category'])

                self.df_domains = df_domains
                
                # Retrieve embedding vectors
                count = 0
                for idx, row in df_domains.iterrows():
                    if row[0] in self.wv.index_to_key:
                        count += 1
                        df_cat.loc[len(df_cat)] = [row[0]] + self.wv[row[0]].tolist() + [dict_domains[row[1]]]
                    else:
                        print(f"Word '{row[0]}' NOT found!")
                        continue
                
                # Performn cluster analysis
                kmeans = KMeans(n_clusters=n_categories, n_init=25, random_state=0).fit(df_cat.copy().drop(['word', 'category'], axis=1).values)
                df_cat.loc[:, 'cluster'] = kmeans.labels_

                self.count = count
                self.df_cat = df_cat

                # Compute purity
                k_purity = {}
                for k in df_cat.cluster.unique():
                    mask_cluster = (df_cat['cluster'] == k)
                    k_purity[k] = df_cat.loc[mask_cluster]['category'].value_counts().values[0] / df_cat.loc[mask_cluster].shape[0]

                end = timeit.default_timer()
                print(f"--> DONE {int((end - start))} second(s)!")

                return sum(k_purity.values())/n_categories
            
            else:
                raise NameError(f"{category_file} is NOT a valid argument, please provide the path to a word category file!")

        except:
            print(format_exc())
            raise Exception


    def Analogy(self, words: bool or str or None, bats: str or None, analogy_domain: list or None, sample: int, seed=100, metric='3cosADD', epsilon=0.001):
        
        start = timeit.default_timer()
        try:
            # Read file
            if words and not bats and not analogy_domain:
                if os.path.isfile(words):
                    with open(words, 'r') as f:
                        words = [[line.strip().split(" ")] for line in f]
                else:
                    url = "https://raw.githubusercontent.com/nicholas-leonard/word2vec/master/questions-words.txt"
                    words = pd.read_csv(StringIO(re.sub(':.*?\n','', requests.request('GET', url).text,
                                            flags=re.DOTALL)), sep=" ", names=['a_word', 'b_word', 'c_word', 'd_word'])
            
            elif bats and not words:
                
                try:
                    bats_wd_analogy = open(bats, "rb")
                    bats_wd_dict = pickle.load(bats_wd_analogy)
                    bats_wd_analogy.close()

                except:
                    raise NameError(f"{bats} argument is NOT valid, please provide a path to the BATS dataset!")
                    
                try:
                    if len(analogy_domain) > 1:
                        words = pd.DataFrame(columns=['a_word', 'b_word', 'c_word', 'd_word'])
                        
                        for d in analogy_domain:   
                            words = pd.concat([words, bats_wd_dict[d]], axis=0)
                    else:
                        words = bats_wd_dict[analogy_domain[0]]

                except:
                    raise ValueError(f"{analogy_domain} is NOT a valid argument, please provide a correct list with the domains!")
                    
            # Sample dataset
            words = words.sample(n=sample, random_state=seed)
            x_words, y_words = (words[['a_word', 'b_word', 'c_word']].values.tolist(), words['d_word'].values.tolist())
            
            df_analogies = pd.DataFrame(columns=['a_word', 'b_word', 'c_word', 'd_word', 'd_pred'])
            
            # Iterate to compute cosine similarity
            for i in range(len(y_words)):
                
                if len([j for j in x_words[i] if j in self.wv.index_to_key]) == 3:
                    vect_a = self.wv[x_words[i][0]]
                    vect_b = self.wv[x_words[i][1]]
                    vect_c = self.wv[x_words[i][2]]
                    
                    max_sim = -99999
                    for w in self.wv.index_to_key:
                        w_vect = self.wv[w]
                        
                        if metric == '3cosADD':
                            similarity = 1 - spatial.distance.cosine(w_vect, (vect_b - vect_a + vect_c))
                        elif metric == '3cosMUL':
                            similarity = (1 - spatial.distance.cosine(w_vect, vect_c))*(1 - spatial.distance.cosine(w_vect, vect_b))/(1 - spatial.distance.cosine(w_vect, vect_a) + epsilon)
                        else:
                            raise ValueError("Please provide one value for 'metric' among: ['3cosADD', '3cosMU']")
                        
                        if similarity > max_sim and w.lower() not in list(map(lambda x: x.lower(), x_words[i])):
                            max_sim, pred = (similarity, w.lower())
                        else:
                            continue

                    df_analogies.loc[len(df_analogies)] = x_words[i] + [y_words[i]]  + [pred]
                    
                else:
                    print(f"Set of words '{x_words[i]} not in vocabulary!'")
                    continue

            df_analogies.loc[:, 'flag'] = df_analogies[['d_word','d_pred']].apply(lambda x: 1 if x[0].lower().strip() == x[1].lower().strip() else 0, axis=1)
            self.df_analogies = df_analogies

            end = timeit.default_timer()
            print(f"--> DONE {int((end - start))} second(s)!")
            
            return df_analogies['flag'].sum() / df_analogies.shape[0]
        
        except:
            print(format_exc())
            raise Exception

    
    def SentimentAnalysis(self, labeled_dataset: str or pd.DataFrame, sample: int or None, seed=100 or None, metrics='accuracy', preprocessing=True):
        
        start = timeit.default_timer()
        try:
            # Read file
            if os.path.isfile(labeled_dataset):
                with open(labeled_dataset, 'r') as f:
                    if labeled_dataset.endswith(".csv") or labeled_dataset.endswith(".txt"):
                        data = pd.read_csv(labeled_dataset)
                    elif labeled_dataset.endswith(".xlsx"):
                        data = pd.read_excel(labeled_dataset)
                    else:
                        raise NameError(f"{labeled_dataset} is NOT a valid argument, please provide a path to a '*.csv', '*.txt' or '*.xlsx' labeled file!")
                
            elif type(labeled_dataset) == pd.DataFrame:
                data = labeled_dataset

            else:
                raise NameError(f"{labeled_dataset} is NOT a valid argument, please provide a path to a labeled file or a DataFrame!")

            # Clean data
            if preprocessing:
                data.loc[:, 'review'] = data['review'].apply(lambda x: x.lower().strip().replace("<br /><br />", " "))
                data.loc[:, 'review'] = data['review'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))
                data.loc[:, 'review'] = data['review'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))
                data.loc[:, 'review'] = data['review'].apply(lambda x: x.translate(str.maketrans('', '', string.digits)))
                data.loc[:, 'review'] = data['review'].apply(lambda x: " ".join([w for w in x.split(" ") if w and w not in self.stopwd]))

            if sample and seed:
                data = data.sample(n=sample, random_state=seed)

            df = pd.DataFrame(columns=['review'] + [f"d_{i}" for i in range(self.vector_size)] + ['Sentiment'])
            
            count_not_found = 0
            count_total_words = 0
            # Compute word-representative vectors
            for idx, row in data.iterrows():
                w_vct = np.zeros(self.vector_size)

                for w in row[0].split(" "):
                    count_total_words += 1
                    if w and w in self.wv.index_to_key:
                        #w_vct += self.wv[w.lower()] * self.wv.get_vecattr(w.lower(), "count")
                        w_vct += self.wv[w] * row[0].count(w)
                    else:
                        count_not_found += 1
                        continue

                df.loc[len(df)] = [row[0]] + w_vct.tolist() + [row[1]]

            # Perform 10-Fold Cross-Validation with Logistic Regression
            logreg = LogisticRegressionCV(cv=10, solver='liblinear', fit_intercept=True, random_state=0, scoring=metrics)
            logreg.fit(df.copy().drop(['review', 'Sentiment'], axis=1), df['Sentiment'])

            # Train BOW baseline model
            vectorizer = CountVectorizer()
            bow = vectorizer.fit_transform(df['review'].values.tolist())
            df_bow = pd.concat([pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names()), df['Sentiment']], axis=1)
            bow_logreg = LogisticRegressionCV(cv=10, solver='liblinear', fit_intercept=True, random_state=0, scoring=metrics)
            bow_logreg.fit(df_bow.copy().drop(['Sentiment'], axis=1), df_bow['Sentiment'])

            #score = logreg.score(df.copy().drop(['review', 'Sentiment'], axis=1), df['Sentiment'])
            #bow_score = bow_logreg.score(df_bow.copy().drop(['Sentiment'], axis=1), df_bow['Sentiment'])
            
            score = round(np.mean(logreg.scores_['positive']), 3)
            bow_score = round(np.mean(bow_logreg.scores_['positive']), 3)
            
            self.score = score
            self.bow_score = bow_score

            end = timeit.default_timer()
            print(f"Percentage of total words NOT found: {count_not_found/count_total_words}")
            print(f"--> DONE {int((end - start))} second(s)!")

            return (self.score, self.bow_score)

        except:
            print(format_exc())
            raise Exception
    
    
    def FrequencyCategory(self, sample_size: int, threshold_list=[100, 1000, 10000], metrics='accuracy'):
        
        start = timeit.default_timer()
        try:
            # Sample word vectors
            if sample_size:
                words = random.sample(self.wv.index_to_key, sample_size)
            else:
                words = self.wv.index_to_key

            scores = []
            for t in threshold_list:
                
                df = pd.DataFrame(columns=['word'] + [f"d_{i}" for i in range(self.vector_size)])
                
                # Normalize vectors
                for i in words:
                    norm_vct = self.wv[i] / np.linalg.norm(self.wv[i])
                    df.loc[len(df)] = [i] + norm_vct.tolist()

                df.loc[:, 'label'] = df['word'].apply(lambda x: 'rare' if self.wv.get_vecattr(x, "count") <= t else 'frequent')            
                
                # Perform 5-Fold Cross-Validation with Logistic Regression
                logreg = LogisticRegressionCV(cv=5, solver='liblinear', fit_intercept=True, random_state=100, 
                                            scoring=metrics, class_weight='balanced')
                logreg.fit(df.copy().drop(['word', 'label'], axis=1), df['label'])
                scores.append((t, round(np.mean(logreg.scores_['rare']), 3)))

            end = timeit.default_timer()
            print(f"--> DONE {int((end - start))} second(s)!")

            return scores

        except:
            print(format_exc())
            raise Exception
    